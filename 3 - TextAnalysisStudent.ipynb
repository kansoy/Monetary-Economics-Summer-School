{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"SummerSchool","language":"python","name":"summerschool"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"name":"3 - TextAnalysisStudent.ipynb","provenance":[{"file_id":"1u1U6fUjvWFUotVgq0U65X6NyQsweuIqq","timestamp":1631204919110}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"7d1b45ef"},"source":["# Text Analysis for Central Bank Communication"],"id":"7d1b45ef"},{"cell_type":"markdown","metadata":{"id":"37b584b5"},"source":["## Initial steps"],"id":"37b584b5"},{"cell_type":"code","metadata":{"id":"e9f890ad"},"source":["# Initial imports\n","\n","import numpy as np\n","import pandas as pd\n","import gensim\n","import os\n","import io\n","import logging\n","from matplotlib import pyplot as plt\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","!pip install pyLDAvis"],"id":"e9f890ad","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6G4KUcdKPldn"},"source":["from google.colab import files\n","uploaded = files.upload() # use widget to upload your file"],"id":"6G4KUcdKPldn","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"of-e4uBZPrjT"},"source":["# Import data\n","df = pd.read_csv(io.StringIO(uploaded['FOMCtext.csv'].decode('utf-8')))"],"id":"of-e4uBZPrjT","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"73b3e272"},"source":["df.head()"],"id":"73b3e272","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0ffd8850"},"source":["This dataframe doesn't look very nice, lets clean it up first"],"id":"0ffd8850"},{"cell_type":"code","metadata":{"id":"04ddc453"},"source":["# Clean up dataframe\n","df = df.loc[:, ['Date', 'Text']] # only want these columns\n","df.loc[:, 'PandasDateTime'] = pd.to_datetime(df.loc[:, 'Date'], format='S_%Y%m%d.txt') # create a proper datetime column"],"id":"04ddc453","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cf3ea7e4"},"source":["df.head()"],"id":"cf3ea7e4","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"25c4687b"},"source":["## Pre-processing text"],"id":"25c4687b"},{"cell_type":"markdown","metadata":{"id":"194e2628"},"source":["Text data is often (always?) messy. Lets take a look at the first entry"],"id":"194e2628"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"fca1f26d","executionInfo":{"status":"ok","timestamp":1631202763083,"user_tz":-60,"elapsed":175,"user":{"displayName":"Tim Munday","photoUrl":"","userId":"14014659251596627992"}},"outputId":"2f5edd53-baca-4d6f-c1e7-eccc026ccb9a"},"source":["df.Text[0]"],"id":"fca1f26d","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"Release Date: February 4, 1994\\n \\n\\nFor immediate release\\nChairman Alan Greenspan announced today that the Federal Open Market Committee decided to increase slightly the degree of pressure on reserve positions. The action is expected to be associated with a small increase in short-term money market interest rates.\\n\\nThe decision was taken to move toward a less accommodative stance in monetary policy in order to sustain and enhance the economic expansion.\\n\\nChairman Greenspan decided to announce this action immediately so as to avoid any misunderstanding of the Committee's purposes, given the fact that this is the first firming of reserve market conditions by the Committee since early 1989.\\n\\n\""]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"c2277fcd"},"source":["What things might we want to strip out of this text to make it less noisy data?\n","\n","Gensim provides an easy way for us to preprocess the text before we do any analysis on it. Below we define a function that will take in the raw text column from our dataframe, and spit out a set of processed documents. The cell below imports the different options you have for preprocessing the text. You should decide which functions you want to add to your preprocessing function.\n","\n","Furthermore, you should use the lambda functions to replace specific parts of the text that you want to modify.\n"],"id":"c2277fcd"},{"cell_type":"code","metadata":{"id":"cf1efaef"},"source":["# Import gensim preprocessing functions\n","from gensim.parsing.preprocessing import preprocess_string # overall function that will allow us to do our preprocessing\n","\n","#\n","from gensim.parsing.preprocessing import remove_stopwords # this removes stopwords like 'and' and 'the'\n","from gensim.parsing.preprocessing import stem_text # this stems text to its root, so economics and econmist both become economi\n","from gensim.parsing.preprocessing import strip_punctuation # this removes punctuation\n","from gensim.parsing.preprocessing import strip_short # this removes words shorter than a length the user provides (default 3)\n","from gensim.parsing.preprocessing import strip_numeric # this removes numbers"],"id":"cf1efaef","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3bc1472e"},"source":["# Define a function that preprocesses the text\n","\n","def Prep(docseries):\n","    '''Function that does some preprocessing\n","    INPUTS: docseries - pandas series, each element is one long string not broken up by tokens\n","    OUTPUTS: result - list of lists, a list of documents each with a list of token strings'''\n","    result=[]\n","    filters = [lambda x: x.replace('date', ''), # fill in other lambda functions\n","               # fill\n","               # fill\n","               #\n","               # fill in other gensim processing functions\n","               ]\n","    result = [preprocess_string(document, filters=filters) for document in docseries.tolist()]\n","    return result"],"id":"3bc1472e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b6251357"},"source":["# Use your function to preprocess the text\n","df['PreppedText'] = Prep(df.Text)"],"id":"b6251357","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"85450572"},"source":["# Look at the new dataframe\n","df.head()"],"id":"85450572","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f76f332e"},"source":["# Look at the new processed text\n","df.PreppedText[0]"],"id":"f76f332e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"46b0d002"},"source":["## Dictionary methods"],"id":"46b0d002"},{"cell_type":"markdown","metadata":{"id":"91ca64c9"},"source":["We can just count words if we want a measure of what the FOMC is talking about. Try 'alan', 'asset', 'basis' etc etc"],"id":"91ca64c9"},{"cell_type":"code","metadata":{"id":"99cf4824"},"source":["df.loc[:, 'word_count'] = df.loc[:, 'PreppedText'].apply(lambda x: x.count('asset')) # "],"id":"99cf4824","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"725bfbd0"},"source":["# Plot results\n","\n","plt.plot(df.PandasDateTime, df.word_count)\n","plt.show()"],"id":"725bfbd0","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"099df52c"},"source":["This is probably OK to use now. But we might be able to do better. Want we are trying to do is increase the signal:noise ratio in the text. A lot of what we just did is remove noise. E.g. fullstops are not useful to us, and so keeping them in adds noise to the dataset, so we removed them. \n","\n","But now it looks like there are words that are going to be extremely common across all FOMC statements. Words like 'release', if you didn't remove them in preprocessing, occur in every statement. And words like 'monetary' probably occur in nearly all of them. These words aren't going to tell us anything because there is no variation in them between statements. Let's try to remove them.\n"],"id":"099df52c"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"46a61020","executionInfo":{"status":"ok","timestamp":1631202771030,"user_tz":-60,"elapsed":203,"user":{"displayName":"Tim Munday","photoUrl":"","userId":"14014659251596627992"}},"outputId":"02a5f294-1b6e-417f-a998-f715b39e614b"},"source":["# Remove rare and common tokens.\n","from gensim.corpora import Dictionary\n","\n","# Create a dictionary representation of the documents.\n","dictionary = Dictionary(df.PreppedText.to_list()) # be clear on what types of objects your function takes\n","\n","# Filter out words that occur less than 3 documents, or more than 50% of the documents.\n","dictionary.filter_extremes(no_below=# fill in, no_above= # fill in)"],"id":"46a61020","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2021-09-09 15:52:51,192 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n","2021-09-09 15:52:51,228 : INFO : built Dictionary(1317 unique tokens: ['1989', '1994', 'accommod', 'action', 'alan']...) from 147 documents (total 31305 corpus positions)\n","2021-09-09 15:52:51,232 : INFO : discarding 362 tokens: [('1989', 1), ('action', 139), ('avoid', 1), ('chairman', 115), ('committe', 146), ('decid', 123), ('econom', 141), ('expect', 105), ('fact', 1), ('feder', 147)]...\n","2021-09-09 15:52:51,234 : INFO : keeping 955 tokens which were in no less than 2 and no more than 102 (=70.0%) documents\n","2021-09-09 15:52:51,238 : INFO : resulting dictionary: Dictionary(955 unique tokens: ['1994', 'accommod', 'alan', 'announc', 'associ']...)\n"]}]},{"cell_type":"code","metadata":{"id":"14df639e"},"source":["# Bag-of-words representation of the documents.\n","corpus = [dictionary.doc2bow(doc) for doc in df.PreppedText.to_list()]"],"id":"14df639e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"879c4e9d"},"source":["print('Number of unique tokens: %d' % len(dictionary))\n","print('Number of documents: %d' % len(corpus))"],"id":"879c4e9d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8930362a"},"source":["## LDA"],"id":"8930362a"},{"cell_type":"markdown","metadata":{"id":"d1dd8346"},"source":["LDA is a generative model. In other words LDA assumes a specific mechanism through which each paragraph is created:\n","\n","• For each topic (k = 1, 2...K), independently draw a distribution over words ($β_k$) from a Dirichlet($η$)\n","distribution\n","\n","• For each paragraph (p = 1, 2...P), independently draw a distribution over topics ($θ_p$) from a\n","Dirichlet($α$) distribution\n","\n","• Then for each word in a given paragraph ($w_n$,p):\n","\n","– Randomly draw a topic ($z_n$,p) from the topic distribution assigned to that paragraph ($θ_p$)\n","\n","– Randomly draw a word from that topic’s distribution of words ($β_zn$,p)\n","\n","This process defines a probability distribution over the data that we observe. Then our problem is to compute the posterior distributions of words for each topic ($β_k$ for all k), and the topic distribution for every paragraph ($θ_p$ for all p), given the words we observe."],"id":"d1dd8346"},{"cell_type":"code","metadata":{"id":"1612cbd0"},"source":["# Train LDA model\n","from gensim.models import LdaModel\n","\n","# Set training parameters.\n","num_topics =  # number of topics\n","chunksize = # how many docs to put in RAM\n","passes = # training epochs\n","iterations = 800\n","eval_every = 0\n","\n","# Make a index to word dictionary.\n","temp = dictionary[0]  # This is only to \"load\" the dictionary.\n","id2word = dictionary.id2token\n","\n","model = LdaModel(\n","    corpus=corpus,\n","    id2word=id2word,\n","    chunksize=chunksize,\n","    alpha='auto',\n","    eta='auto',\n","    iterations=iterations,\n","    num_topics=num_topics,\n","    passes=passes,\n","    eval_every=eval_every\n",")"],"id":"1612cbd0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f2743bec"},"source":["import pyLDAvis\n","import pyLDAvis.gensim_models as gensimvis\n","pyLDAvis.enable_notebook()\n","\n","lda_viz = gensimvis.prepare(model, corpus, dictionary)"],"id":"f2743bec","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cfa20b6b"},"source":["# Do visualisation\n","lda_viz"],"id":"cfa20b6b","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"212451de"},"source":["## Word2Vec"],"id":"212451de"},{"cell_type":"markdown","metadata":{"id":"c718a39a"},"source":["Sometimes we are not so interested in just reducing dimensionality. We might want to find words similar to another word, for example. Tasks like these require us to retain the relationships between words (i.e. we cannot throw that information away by using Bag of Words models)."],"id":"c718a39a"},{"cell_type":"markdown","metadata":{"id":"66df52e6"},"source":["Neural network architectures for NLP are now common. We will review a famous one, word2vec.\n","\n","Word2vec (in our case) does two things:\n","1. Asks a shallow neural network topredict a target word, given the context of the word (i.e. the words in a small window around the target word)\n","2. After training the network, the weights corresponding to the hidden layer are extracted and used as vector representations of words"],"id":"66df52e6"},{"cell_type":"code","metadata":{"id":"a4b39dba"},"source":["w2vmodel = gensim.models.Word2Vec(df.PreppedText,\n","                               min_count=,        # ignore words with frequency less than this\n","                               window = ,  # window size around word to predict\n","                               size=,    # length of vector # new version: vector_size\n","                               workers=,          # number of cores to use\n","                               iter=,          # number of times to iterate over the corpus # new version: epochs\n","                               negative=5)  # negative sub sampling parameter"],"id":"a4b39dba","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f1b448dc"},"source":["# Does it work?\n","w2vmodel.wv.most_similar('alan')"],"id":"f1b448dc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cwn42CtgWksB"},"source":[""],"id":"cwn42CtgWksB","execution_count":null,"outputs":[]}]}