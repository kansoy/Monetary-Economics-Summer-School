{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"SummerSchool","language":"python","name":"summerschool"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"name":"2 - MonPolShocks-Student.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"dee1ec73"},"source":["# Monetary policy shocks"],"id":"dee1ec73"},{"cell_type":"markdown","metadata":{"id":"fa76c62b"},"source":["Note that first you will need to download the dataset from  https://www.aeaweb.org/articles?id=10.1257/mac.20180090"],"id":"fa76c62b"},{"cell_type":"markdown","metadata":{"id":"e61c1a7f"},"source":["## Initial steps"],"id":"e61c1a7f"},{"cell_type":"code","metadata":{"id":"22a96140"},"source":["# Import packages\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","import statsmodels.api as sm\n","from statsmodels.tsa.api import VAR\n","import io"],"id":"22a96140","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kmx5eHUwuJG3"},"source":["from google.colab import files\n","uploaded = files.upload() # use widget to upload your file"],"id":"Kmx5eHUwuJG3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"20946e3a"},"source":["# Import data (from Jarocinski and Karadi 2020, AEJ:Macro)\n","df = pd.read_csv(io.StringIO(uploaded['FILENAMEHERE.csv'].decode('utf-8')))"],"id":"20946e3a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7e80ffae"},"source":["# View data\n","\n","df.head()"],"id":"7e80ffae","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f88b59be"},"source":["Looks like we have a lot of NAs and a lot of unnecessary columns - let's remove them.\n","The columns we want are 'year', 'month', 'ff4_hf', 'sp500_hf', 'logsp500', 'gs1', 'us_rgdp', 'us_gdpdef'. Most of these names are self explanatory. Others aren't: ff4_hf is the high frequency surprise in fed funds futures, sp500_hf is the high frequency surprise in the stock market, gs1 is a short term interest rate measure."],"id":"f88b59be"},{"cell_type":"code","metadata":{"id":"d935d55d"},"source":["# Wrangle data: remove unnecessary columns\n","\n","column_list = ['year', 'month', 'ff4_hf', 'sp500_hf', 'logsp500', 'gs1', 'us_rgdp', 'us_gdpdef'] # these are the columns we want to keep\n","df = df.loc[:, column_list]"],"id":"d935d55d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c44680db"},"source":["# Plot the data\n","\n","plt.plot(df.us_gdpdef)\n","plt.show()\n","\n","plt.plot(df.us_rgdp)\n","plt.show()\n","\n","plt.plot(df.gs1)\n","plt.show()"],"id":"c44680db","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"71d96ebc"},"source":["Whats the problem with the first two graphs? If we think the data is non-stationary, we should alter it for our VARs.\n","\n","In the cell below:\n","1. Decide if you want to transform the data, and if so, how. (Hint: consult documentation on pandas.series.diff)\n","2. Restrict the dataframe to just the (transformed) cpi, ip and ff4_hf, and call the new dataframe df_data\n","3. Drop any rows that contain NAs from the dataframe (Hint: consult documentation on pandas.dropna)"],"id":"71d96ebc"},{"cell_type":"code","metadata":{"id":"a5b988c7"},"source":["# -- #\n"],"id":"a5b988c7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c4b126e5"},"source":["## VARs"],"id":"c4b126e5"},{"cell_type":"code","metadata":{"id":"fa852ac3"},"source":["# Run a VAR\n","model = VAR(df_data)\n","results = model.fit(4) # 4 is the number of lags we use"],"id":"fa852ac3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2dee2680"},"source":["# Print a summary of the results\n","results.summary()"],"id":"2dee2680","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bf3b9b41"},"source":["# Create irfs\n","irf = results.irf(20)"],"id":"bf3b9b41","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"837adceb"},"source":["# Plot irfs\n","plot = irf.plot(orth=True, impulse='ff4_hf')"],"id":"837adceb","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8146ff02"},"source":["Now you know how to run a basic VAR in python. In the cells below:\n","1. Add controls to your dataframe, including the surprises in the S&P500, the measure of interest rates gs1, the US real GDP, US GDP deflator, and log S&P 500. Remember to transform data appropriately if you think it is non-stationary\n","2. Run a VAR with the surprises ordered first, followed by gs1, followed by the S&P500, followed by real GDP, and finally the US GDP deflator (why would this ordering be sensible? What does it imply about the information set of the central bank. This is what J&K use)"],"id":"8146ff02"},{"cell_type":"code","metadata":{"id":"027ae2bd"},"source":["# -- # \n"],"id":"027ae2bd","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b17452de"},"source":["This should now look pretty similar to Jarocinksi and Karadi's VAR in Panel B of page 13. It won't be exactly the same as we have (i) not imposed the exact restrictions they have on the HF shocks, (ii) not included all of the controls, and (iii) depending on how we removed stationarity may have slightly different variables - indeed they leave the variables in log-levels.\n","\n","There's not much going on in this VAR - which is why JK use sign-restrictions to try to get at monetary policy shocks versus information shocks. After doing that they find results more in line with what theory would predict."],"id":"b17452de"},{"cell_type":"markdown","metadata":{"id":"f0edf38c"},"source":["Now: In the cells below, run a VAR with the ordering reversed (this is a more standard ordering - why?)"],"id":"f0edf38c"},{"cell_type":"code","metadata":{"id":"f6258252"},"source":["# -- #"],"id":"f6258252","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4d2ab5ad"},"source":["There was at one point a 'price puzzle' in VARs, in which interest rates looked like they increased inflation. This puzzle occured a while ago. Let's see if we can find it in the data. \n","\n","In the cells below:\n","Run a three variable VAR of gs1, us_rgdp, us_gdpdef, all untransformed (i.e. estimated in log levels), and with interest rates ordered first. But - restrict the VAR to data occuring to 2000 and before..."],"id":"4d2ab5ad"},{"cell_type":"code","metadata":{"id":"d44705e0"},"source":["# -- #"],"id":"d44705e0","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0da3a44e"},"source":["## Local projections"],"id":"0da3a44e"},{"cell_type":"markdown","metadata":{"id":"4b35de2e"},"source":["In the lectures you will have seen what local projections are. In the cells below\n","1. Create a dataframe containing ff4_hf, us_rgdp (transformed however), and sp500_hf\n","2. Run local projections to create an \"impulse responses\" of length 20 of the effect of ff4_hf on US real GDP, including confidence intervals (Hint: you will need a for loop, and sm.OLS() to run OLS regressions)\n","3. Then run a VAR with the same variables and check that the (qualitative) answers are similar"],"id":"4b35de2e"},{"cell_type":"code","metadata":{"id":"2ea52565"},"source":["# -- # \n","\n"],"id":"2ea52565","execution_count":null,"outputs":[]}]}